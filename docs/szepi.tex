\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}

\usepackage{amsmath, amsthm, amssymb}

\newcommand{\cF}{\mathbf{F}}
\newcommand{\cX}{\mathbf{X}}
\newcommand{\one}{1}
\newcommand{\EE}{\mathbf{E}}

\begin{document}


Szia!
Reggeli csomagbontás!

Leírok egy pár triviális dolgot, amik könnyen jönnek, mert kiváncsi vagyok, hogy mi is történik.

Az EMD definíciója a következő. (Az ékezeteket kikapcsolom a továbbiakban, mert akkor nem tudok LaTeX-et gépelni. De lehet, kicsit összevissza lesz.. Bocs..)
Legyen $(X,d)$ az alaptér, ami egy metrikus ter es legyen $\cF$ egy kelloen gazdag halmaza az $X\to X$ fuggvenyeknek (pl. az osszes Lipschitz fuggveny, az osszes 3 retegu neuronhaloval reprezentalhato fuggveny, stb.)
A tovabbiakban az EMD-t $\cF$-re nezve definialjuk, de a jelolesben $\cF$-et kidobjuk majd.
Legyen $P,Q$ ket eloszlas $X$ felett. Definicio szerint a $P,Q$ kozotti EMD tavolsag

$D(P,Q) = \inf_{f\in \cF} \int d( x, f(y) ) P(dx) \,Q(dy)$.

(Az asszimetria azt gondolnám, hogy csak látszólagos, ha $f$ elég gazdag.
Ami ide van irva egyebkent az nem egeszen az EMD vagy "Optimal Transport" distance, hanem az un. Monge-féle változat:
https://en.wikipedia.org/wiki/Transportation\_theory\_%28mathematics%29.
Ez jonak tunik:
https://www.math.ucdavis.edu/~qlxia/Research/monge.pdf
De ne foglalkozzunk ilyen kicsiségekkel most..)

Egy lehetseges modszer arra, hogy egy $P$ eloszlást közelítsen az ember az lehetne az, hogy az ember rögzít egy egyszerű eloszlást (normális, mint nálad, vagy egyenletes, stb.) és kiszámolja a minimalizáló $f\in \cF$ leképezést a fenti képletben:

$f^*_{P,Q} = \arg\min_{f\in \cF} \int d(x,f(y)) P(dx) Q(dy)$.

Ha $P$-ből csak egy $X_1,\dots,X_n \sim P$ iid minta van, akkor kicserélhetjük $P$-t a fentiekben a $P_n(dx) = \frac1n \sum_{i=1}^n \one{X_i\in dx}$ empirikus eloszlásra:

$f^*_{P_n,Q} = \arg\min_{f\in \cF} \int d(x,f(y)) P_n(dx) Q(dy) = \arg\min_{f\in \cF} \frac1n \sum_{i=1}^n \int d(X_i,f(y)) Q(dy) $.

Ezt még mindig nem igazán számolható. A következő gondolat, hogy $Q$-t is közelítse az ember egy empirikus eloszlással ($Q_m(dy) = \frac1m \sum_{j=1}^m \one{Y_j\in dy}$, $Y_1,\dots,Y_m \sim Q$ iid):

$f^*_{P_n,Q_m} = \arg\min_{f\in \cF} \int d(x,f(y)) P_n(dx) Q_m(dy) = \arg\min_{f\in \cF} \frac1{nm} \sum_{i=1}^n \sum_{j=1}^m d(X_i,f(Y_j)) $.

Persze rögtön felmerül a kérdés, hogy mekkora legyen $m$. Egy heurisztika az, hogy az $m$ mintával való approximáció ne okozzon se kisebb, se nagyob hibát, mint a $P$ $n$ mintával való approximációja. Így az ember arra gondolhat, hogy $m \approx n$ jó választás lehet. Általánosabban, talán azt kellene mondani, hogy legyen $m \ge n$.

Btw, itt meg is állhatnánk egy pillanatra. Ez lenne az empirikus riszk (hehe) minimalizáció mintájára épített EMD alapú eloszlás tanulás.
No jó, inkább "optimal transport" alapú distribution learning. És a tied "neural" distribution learning, hihi.

Miért álljunk itt meg? Azért mert ha pl. gradiens módszerrel akarja az ember az argmin-t számolni, akkor lehetne a gradienst mintavételezni, és nem kellene drága assignment-et számolni. Ha

$L_n(f) := \frac1{nm}\sum_{i=1}^n \sum_{j=1}^m d(X_i,f(Y_j)) $,

akkor pl. $\nabla_f L_n(f) = \EE{ \nabla_f d(X_{I},f(Y_J)) }$, ha $I$ es $J$ egyenletes eloszlású véletlen indexek. (Itt a gradienst hogy tudjuk normálisan definiálni több lehetőség is van, pl. $\cF$-et metrikus térré tenni, és akkor lehet a Frechet deriváltakat venni; de azt is lehet, hogy vesz az ember egy véges dimenziós parametrizálást és akkor a paraméterekre veszi az ember a deriváltat. Lényegében a neuronhálós megoldás az ilyen. A különbségre pl. Amari mutat rá, innen származtatható a "natural gradient".)

Nem mondom, hogy ez feltétlenül a legjobb módszer, de kár lenne elmenni mellette.

Most, hogy kicsit gondolkodtam rajta, úgy tűnik hogy a megfelelő kereső kulcsszavak "optimal transport" (mert az EMD-t ugye ezzel számolják és valahogy népszerűbb) és "probability distribution learning", ami fel is dobja, pl. ezt a cikket (van több is):

http://arxiv.org/pdf/1209.1077v1.pdf
G. Canas and L. Rosasco. Learning probability measures with respect to optimal transport metrics. In Adv. in Neural Infor. Proc. Systems 25 ,pages 2501–2509. 2012.

Nagyon relevánsnak tűnik, de elolvasva kicsit mást csinál: egy eloszlást egy véges ponthálóval próbál közelíteni.

Viszont azért úgy tűnik ez is meglehetősen releváns -- speciel arra a megközelítésre nézve amit ti találtatok ki.

Hogy lehetne formálisan leírni azt amit csináltok?

Továbbra is lesz valami $f\in \cF$ leképezés és egy $Y_1,\dots,Y_m\sim Q$ iid pontsereg.
Az $f$ és a pontsereg együtt ad egy $S_{f,m} = \{f(Y_1),\dots,f(Y_m)\}$ véletlen halmazt, és ha $S\subset \cX$-re $d(x,S) = \inf_{s\in S} d(x,s)$, akkor ugye a módszerre lehet úgy gondolni (eltekintve a minibatchtől), hogy az

$L_m(f) = \EE{\int d(x, S_{f,m} ) P(dx)}$

költség empirikus közelítését minimalizálja. Speciel, ha

$L_{n,m}(f) = \int d(x, S_{f,m} ) P_n(dx) = \frac1n \sum_{i=1}^n d(X_i, S_{f,m} )$,

akkor minden iterációban $f$-et az $\nabla_f L_{n,m}$-val ellentétes irányba löködöd.

Ez egy új megvilágításba helyezi a módszert.
Mint a fenti cikk 3.1-es lemmája mutatja,

$L_m(f) = W_1(P, \pi_{S_{f,m}} P )$,

ahol $\pi_S P$ a $P$ mérték képe a $\pi_S: X \to S$ nearest-neighbor map mellett (azaz $\pi_S P$ egy véletlen $X\sim P$ pont $\pi_S(X)$ képének eloszlása $S$-en), és $W_1(P,Q)$ a Wasserstrein-$1$ távolság a $P$ és $Q$ között, ami lényegében a fenti EMD.

Ha most $m$ tart a végtelenbe, akkor azt látjuk, hogy a módszer az

$L(f) = W_1(P,\pi_{S_f}P)$

hibafüggvényt minimalizálja, ahol $S_f = \{ f(y)\,:\, y\in \mathrm{supp}(Q) \}$.

Így aztán a helyes interpretáció inkább az, hogy a $P$-t a $\pi_{S_f} P$-vel közítjük. Ebből az is kiderült, hogy a $Q$ eloszlásnak csak az a szerepe (aszimptotikusan), hogy a support-ját adja.

Ez inkább azt mutatja, hogy jobb úgy gondolni a módszerre, mint ami "filterez", vagy $P$-t vetíti az $\cF$ függvényhalmaz segítségével.
A másik következmény, hogy az $f(Y)$, $Y\sim Q$ valváltozó nem lesz feltétlenül eloszlásban közel (semmilyen értelemben sem) $P$-hez.
Vagyis a fentiekből nem látszik, hogy ez miért lenne így. De azért az ember azt gondolja, hogy a $Q$-nak mégiscsak kellene valami szerepe legyen. Lehet, hogy az $m$ nem túl nagy az $n$-hez képest, akkor a fenti határátmenetek nem fognak működni és a $Q$ nem esik ki (ez akár jó is lehet, ha valóban eloszlást akar az ember tanulni). Ezt most meghagyom neked:) Btw, lehet, hogy jó lett volna ezt valami latex doksiba gépelni?

Szepi

\end{document}
